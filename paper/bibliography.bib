
@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  volume = {59},
  issn = {0036-1445},
  url = {https://epubs.siam.org/doi/10.1137/141000671},
  doi = {10.1137/141000671},
  shorttitle = {Julia},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be “laws of nature"  by practitioners of numerical computing: \textbackslash{}beginlist \textbackslash{}item  High-level dynamic programs have to be slow. \textbackslash{}item  One must prototype in one language and then rewrite in another language for speed or deployment. \textbackslash{}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \textbackslash{}endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
  number = {1},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  urldate = {2018-10-25},
  date = {2017-01-01},
  pages = {65-98},
  author = {Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.},
  file = {/home/alec/Zotero/storage/J8LCZESF/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf;/home/alec/Zotero/storage/ZV4KKMJY/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf;/home/alec/Zotero/storage/8WDDK9TI/141000671.html;/home/alec/Zotero/storage/URQZFHVI/141000671.html},
  ids = {bezansonJuliaFreshApproach2017a}
}

@article{shankarNeuralMechanismSimulate2016,
  title = {Neural {{Mechanism}} to {{Simulate}} a {{Scale}}-{{Invariant Future}}},
  volume = {28},
  issn = {0899-7667},
  url = {https://doi.org/10.1162/NECO_a_00891},
  doi = {10.1162/NECO_a_00891},
  abstract = {Predicting the timing and order of future events is an essential feature of cognition in higher life forms. We propose a neural mechanism to nondestructively translate the current state of spatiotemporal memory into the future, so as to construct an ordered set of future predictions almost instantaneously. We hypothesize that within each cycle of hippocampal theta oscillations, the memory state is swept through a range of translations to yield an ordered set of future predictions through modulations in synaptic connections. Theoretically, we operationalize critical neurobiological findings from hippocampal physiology in terms of neural network equations representing spatiotemporal memory. Combined with constraints based on physical principles requiring scale invariance and coherence in translation across memory nodes, the proposition results in Weber-Fechner spacing for the representation of both past (memory) and future (prediction) timelines. We show that the phenomenon of phase precession of neurons in the hippocampus and ventral striatum correspond to the cognitive act of future prediction.},
  number = {12},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2019-06-12},
  date = {2016-09-14},
  pages = {2594-2627},
  author = {Shankar, Karthik H. and Singh, Inder and Howard, Marc W.},
  file = {/home/alec/Zotero/storage/76CAIM4A/Shankar et al. - 2016 - Neural Mechanism to Simulate a Scale-Invariant Fut.pdf;/home/alec/Zotero/storage/QVGVY5I5/Shankar et al. - 2016 - Neural Mechanism to Simulate a Scale-Invariant Fut.pdf;/home/alec/Zotero/storage/P69NLNYW/NECO_a_00891.html;/home/alec/Zotero/storage/QH6XFW7T/NECO_a_00891.html},
  ids = {shankarNeuralMechanismSimulate2016a}
}

@article{tiganjSimpleBiophysicallyPlausible2015,
  langid = {english},
  title = {A {{Simple}} Biophysically Plausible Model for Long Time Constants in Single Neurons},
  volume = {25},
  issn = {1098-1063},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.22347},
  doi = {10.1002/hipo.22347},
  abstract = {Recent work in computational neuroscience and cognitive psychology suggests that a set of cells that decay exponentially could be used to support memory for the time at which events took place. Analytically and through simulations on a biophysical model of an individual neuron, we demonstrate that exponentially decaying firing with a range of time constants up to minutes could be implemented using a simple combination of well-known neural mechanisms. In particular, we consider firing supported by calcium-controlled cation current. When the amount of calcium leaving the cell during an interspike interval is larger than the calcium influx during a spike, the overall decay in calcium concentration can be exponential, resulting in exponential decay of the firing rate. The time constant of the decay can be several orders of magnitude larger than the time constant of calcium clearance, and it could be controlled externally via a variety of biologically plausible ways. The ability to flexibly and rapidly control time constants could enable working memory of temporal history to be generalized to other variables in computing spatial and ordinal representations. © 2014 Wiley Periodicals, Inc.},
  number = {1},
  journaltitle = {Hippocampus},
  urldate = {2019-07-09},
  date = {2015},
  pages = {27-37},
  keywords = {calcium-sensitive nonselective cation current,cholinergic modulation,exponentially decaying firing,persistent firing,working memory},
  author = {Tiganj, Zoran and Hasselmo, Michael E. and Howard, Marc W.},
  file = {/home/alec/Zotero/storage/7QNHDDJI/Tiganj et al. - 2015 - A Simple biophysically plausible model for long ti.pdf;/home/alec/Zotero/storage/9VSL9X8X/Tiganj et al. - 2015 - A simple biophysically plausible model for long ti.pdf;/home/alec/Zotero/storage/NF4U3JYH/hipo.html},
  ids = {tiganjSimpleBiophysicallyPlausible2015a}
}

@article{tsaoIntegratingTimeExperience2018,
  langid = {english},
  title = {Integrating Time from Experience in the Lateral Entorhinal Cortex},
  volume = {561},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/s41586-018-0459-6},
  doi = {10.1038/s41586-018-0459-6},
  abstract = {Temporal information that is useful for episodic memory is encoded across a wide range of timescales in the lateral entorhinal cortex, arising inherently from its representation of ongoing experience.},
  number = {7721},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2019-10-30},
  date = {2018-09},
  pages = {57-62},
  author = {Tsao, Albert and Sugar, Jørgen and Lu, Li and Wang, Cheng and Knierim, James J. and Moser, May-Britt and Moser, Edvard I.},
  file = {/home/alec/Zotero/storage/8DKY236A/Tsao et al. - 2018 - Integrating time from experience in the lateral en.pdf;/home/alec/Zotero/storage/9JDLURS9/s41586-018-0459-6.html}
}

@article{shankarScaleinvariantInternalRepresentation2012,
  langid = {english},
  title = {A Scale-Invariant Internal Representation of Time},
  volume = {24},
  issn = {1530-888X},
  doi = {10.1162/NECO_a_00212},
  abstract = {We propose a principled way to construct an internal representation of the temporal stimulus history leading up to the present moment. A set of leaky integrators performs a Laplace transform on the stimulus function, and a linear operator approximates the inversion of the Laplace transform. The result is a representation of stimulus history that retains information about the temporal sequence of stimuli. This procedure naturally represents more recent stimuli more accurately than less recent stimuli; the decrement in accuracy is precisely scale invariant. This procedure also yields time cells that fire at specific latencies following the stimulus with a scale-invariant temporal spread. Combined with a simple associative memory, this representation gives rise to a moment-to-moment prediction that is also scale invariant in time. We propose that this scale-invariant representation of temporal stimulus history could serve as an underlying representation accessible to higher-level behavioral and cognitive mechanisms. In order to illustrate the potential utility of this scale-invariant representation in a variety of fields, we sketch applications using minimal performance functions to problems in classical conditioning, interval timing, scale-invariant learning in autoshaping, and the persistence of the recency effect in episodic memory across timescales.},
  number = {1},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput},
  date = {2012-01},
  pages = {134-193},
  keywords = {Animals,Models; Neurological,Neurons,Neural Networks (Computer),Humans,Algorithms,Learning,Computational Biology,Time Factors,Memory,Conditioning; Classical,Memory; Episodic},
  author = {Shankar, Karthik H. and Howard, Marc W.},
  file = {/home/alec/Zotero/storage/99SAGYF3/Shankar and Howard - 2011 - A Scale-Invariant Internal Representation of Time.pdf;/home/alec/Zotero/storage/6MATX6LX/NECO_a_00212.html},
  ids = {shankarScaleInvariantInternalRepresentation2011},
  eprinttype = {pmid},
  eprint = {21919782}
}

@article{tiganjEncodingLaplaceTransform2013,
  title = {Encoding the {{Laplace}} Transform of Stimulus History Using Mechanisms for Persistent Firing},
  volume = {14},
  issn = {1471-2202},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3704764/},
  doi = {10.1186/1471-2202-14-S1-P356},
  issue = {Suppl 1},
  journaltitle = {BMC Neuroscience},
  shortjournal = {BMC Neurosci},
  urldate = {2019-10-30},
  date = {2013-07-08},
  pages = {P356},
  author = {Tiganj, Zoran and Shankar, Karthik H and Howard, Marc W},
  file = {/home/alec/Zotero/storage/3V7KWDMV/Tiganj et al. - 2013 - Encoding the Laplace transform of stimulus history.pdf},
  eprinttype = {pmid},
  eprint = {null},
  pmcid = {PMC3704764}
}

@book{hilleFunctionalAnalysisSemigroups1957,
  title = {Functional Analysis and Semi-Groups},
  url = {https://mathscinet.ams.org/mathscinet-getitem?mr=0089373},
  pagetotal = {xii+808},
  series = {American {{Mathematical Society Colloquium Publications}}, Vol. 31},
  publisher = {{American Mathematical Society, Providence, R. I.}},
  urldate = {2019-10-30},
  date = {1957},
  author = {Hille, Einar and Phillips, Ralph S.},
  file = {/home/alec/Zotero/storage/NA8XSCLR/mathscinet-getitem.html},
  mrnumber = {0089373}
}

@article{webbFactorisedNeuralRelational2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.08721},
  primaryClass = {cs, stat},
  title = {Factorised {{Neural Relational Inference}} for {{Multi}}-{{Interaction Systems}}},
  url = {http://arxiv.org/abs/1905.08721},
  abstract = {Many complex natural and cultural phenomena are well modelled by systems of simple interactions between particles. A number of architectures have been developed to articulate this kind of structure, both implicitly and explicitly. We consider an unsupervised explicit model, the NRI model, and make a series of representational adaptations and physically motivated changes. Most notably we factorise the inferred latent interaction graph into a multiplex graph, allowing each layer to encode for a different interaction-type. This fNRI model is smaller in size and significantly outperforms the original in both edge and trajectory prediction, establishing a new state-of-the-art. We also present a simplified variant of our model, which demonstrates the NRI's formulation as a variational auto-encoder is not necessary for good performance, and make an adaptation to the NRI's training routine, significantly improving its ability to model complex physical dynamical systems.},
  urldate = {2019-10-30},
  date = {2019-05-21},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Webb, Ezra and Day, Ben and Andres-Terre, Helena and Lió, Pietro},
  file = {/home/alec/Zotero/storage/KHIQ95UL/Webb et al. - 2019 - Factorised Neural Relational Inference for Multi-I.pdf;/home/alec/Zotero/storage/I4SMBKBX/1905.html}
}

@article{debrangesStoneWeierstrassTheorem1959,
  langid = {english},
  title = {The {{Stone}}-{{Weierstrass}} Theorem},
  volume = {10},
  issn = {0002-9939, 1088-6826},
  url = {https://www.ams.org/proc/1959-010-05/S0002-9939-1959-0113131-7/},
  doi = {10.1090/S0002-9939-1959-0113131-7},
  abstract = {Advancing research. Creating connections.},
  number = {5},
  journaltitle = {Proceedings of the American Mathematical Society},
  shortjournal = {Proc. Amer. Math. Soc.},
  urldate = {2019-10-30},
  date = {1959},
  pages = {822-824},
  author = {de Branges, Louis},
  options = {useprefix=true},
  file = {/home/alec/Zotero/storage/WEALHJSW/de Branges - 1959 - The Stone-Weierstrass theorem.pdf;/home/alec/Zotero/storage/HKS6PS78/home.html}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  langid = {english},
  title = {Approximation by Superpositions of a Sigmoidal Function},
  volume = {2},
  issn = {1435-568X},
  url = {https://doi.org/10.1007/BF02551274},
  doi = {10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  number = {4},
  journaltitle = {Mathematics of Control, Signals and Systems},
  shortjournal = {Math. Control Signal Systems},
  urldate = {2019-10-30},
  date = {1989-12-01},
  pages = {303-314},
  keywords = {Neural networks,Approximation,Completeness},
  author = {Cybenko, G.},
  ids = {cybenkoApproximationSuperpositionsSigmoidal1989}
}

@incollection{luExpressivePowerNeural2017,
  title = {The {{Expressive Power}} of {{Neural Networks}}: {{A View}} from the {{Width}}},
  url = {http://papers.nips.cc/paper/7203-the-expressive-power-of-neural-networks-a-view-from-the-width.pdf},
  shorttitle = {The {{Expressive Power}} of {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-10-30},
  date = {2017},
  pages = {6231--6239},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  file = {/home/alec/Zotero/storage/2SH9P2SX/Lu et al. - 2017 - The Expressive Power of Neural Networks A View fr.pdf;/home/alec/Zotero/storage/9YLS9ST7/7203-the-expressive-power-of-neural-networks-a-view-from-the-width.html}
}

@article{haninApproximatingContinuousFunctions2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.11278},
  primaryClass = {cs, math, stat},
  title = {Approximating {{Continuous Functions}} by {{ReLU Nets}} of {{Minimal Width}}},
  url = {http://arxiv.org/abs/1710.11278},
  abstract = {This article concerns the expressive power of depth in deep feed-forward neural nets with ReLU activations. Specifically, we answer the following question: for a fixed \$d\_\{in\}\textbackslash{}geq 1,\$ what is the minimal width \$w\$ so that neural nets with ReLU activations, input dimension \$d\_\{in\}\$, hidden layer widths at most \$w,\$ and arbitrary depth can approximate any continuous, real-valued function of \$d\_\{in\}\$ variables arbitrarily well? It turns out that this minimal width is exactly equal to \$d\_\{in\}+1.\$ That is, if all the hidden layer widths are bounded by \$d\_\{in\}\$, then even in the infinite depth limit, ReLU nets can only express a very limited class of functions, and, on the other hand, any continuous function on the \$d\_\{in\}\$-dimensional unit cube can be approximated to arbitrary precision by ReLU nets in which all hidden layers have width exactly \$d\_\{in\}+1.\$ Our construction in fact shows that any continuous function \$f:[0,1]\^\{d\_\{in\}\}\textbackslash{}to\textbackslash{}mathbb R\^\{d\_\{out\}\}\$ can be approximated by a net of width \$d\_\{in\}+d\_\{out\}\$. We obtain quantitative depth estimates for such an approximation in terms of the modulus of continuity of \$f\$.},
  urldate = {2019-10-30},
  date = {2018-03-10},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Computational Complexity,Mathematics - Combinatorics,Mathematics - Statistics Theory},
  author = {Hanin, Boris and Sellke, Mark},
  file = {/home/alec/Zotero/storage/VWQYK5MA/Hanin and Sellke - 2018 - Approximating Continuous Functions by ReLU Nets of.pdf;/home/alec/Zotero/storage/4WM4JC7M/1710.html}
}

@article{liuGenerationScaleinvariantSequential2019,
  langid = {english},
  title = {Generation of Scale-Invariant Sequential Activity in Linear Recurrent Networks},
  url = {https://www.biorxiv.org/content/10.1101/580522v1},
  doi = {10.1101/580522},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Sequential neural activity has been observed in many parts of the brain and has been proposed as a neural mechanism for memory. The natural world expresses temporal relationships at a wide range of scales. Because we cannot know the relevant scales \emph{a priori} it is desirable that memory, and thus the generated sequences, are scale-invariant. Although recurrent neural network models have been proposed as a mechanism for generating sequences, the requirements for \emph{scale-invariant} sequences are not known. This paper reports the constraints that enable a linear recurrent neural network model to generate scale-invariant sequential activity. A straightforward eigendecomposition analysis results in two independent conditions that are required for scale-invariance. First the eigenvalues of the network must be geometrically spaced. Second, the eigenvectors must be related to one another \emph{via} translation. These constraints, along with considerations on initial conditions, provide a general recipe to build linear recurrent neural networks that support scale-invariant sequential activity.{$<$}/p{$>$}},
  journaltitle = {bioRxiv},
  urldate = {2019-10-31},
  date = {2019-03-17},
  pages = {580522},
  author = {Liu, Yue and Howard, Marc W.},
  file = {/home/alec/Zotero/storage/TKNGXVPJ/Liu and Howard - 2019 - Generation of scale-invariant sequential activity .pdf;/home/alec/Zotero/storage/EFG3LGKV/580522v1.html}
}

@article{howardMathematicalLearningTheory2014,
  langid = {english},
  title = {Mathematical Learning Theory through Time},
  volume = {59},
  issn = {0022-2496},
  url = {http://www.sciencedirect.com/science/article/pii/S0022249613000850},
  doi = {10.1016/j.jmp.2013.09.003},
  abstract = {Stimulus sampling theory~(SST:~ Estes, 1950, Estes, 1955a, Estes, 1955b, Estes, 1959) was the first rigorous mathematical model of learning that posited a central role for an abstract cognitive representation distinct from the stimulus or the response. SST posited that (a) conditioning takes place not on the nominal stimulus presented to the learner, but on a cognitive representation caused by the nominal stimulus, and (b) the cognitive representation caused by a nominal stimulus changes gradually across presentations of that stimulus. Retrieved temporal context models assume that (a) a distributed representation of temporal context changes gradually over time in response to the studied stimuli, and (b) repeating a stimulus can recover a prior state of temporal context. We trace the evolution of these ideas from the early work on SST, and argue that recent neuroscientific evidence provides a physical basis for the abstract models that Estes envisioned more than a half-century ago.},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  series = {Special {{Issue}} in {{Honor}} of {{William K}}. {{Estes}}},
  urldate = {2019-10-31},
  date = {2014-04-01},
  pages = {18-29},
  keywords = {Cognitive neuroscience,Memory models},
  author = {Howard, Marc W.},
  file = {/home/alec/Zotero/storage/ITV2EK5K/Howard - 2014 - Mathematical learning theory through time.pdf;/home/alec/Zotero/storage/77V7K9IU/S0022249613000850.html}
}

@book{shankarOptimallyFuzzyTemporal,
  title = {Optimally {{Fuzzy Temporal Memory}}},
  abstract = {Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register—a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an indepen-dent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availabil-ity. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from expo-nentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system.},
  author = {Shankar, Karthik H. and Howard, Marc W. and Dayan, Peter},
  file = {/home/alec/Zotero/storage/9DD4QIEH/Shankar et al. - Optimally Fuzzy Temporal Memory.pdf;/home/alec/Zotero/storage/RT5U9G79/summary.html}
}

@article{howardUnifiedMathematicalFramework2014,
  title = {A {{Unified Mathematical Framework}} for {{Coding Time}}, {{Space}}, and {{Sequences}} in the {{Hippocampal Region}}},
  volume = {34},
  issn = {0270-6474},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3965792/},
  doi = {10.1523/JNEUROSCI.5808-12.2014},
  abstract = {The medial temporal lobe (MTL) is believed to support episodic memory, vivid recollection of a specific event situated in a particular place at a particular time. There is ample neurophysiological evidence that the MTL computes location in allocentric space and more recent evidence that the MTL also codes for time. Space and time represent a similar computational challenge; both are variables that cannot be simply calculated from the immediately available sensory information. We introduce a simple mathematical framework that computes functions of both spatial location and time as special cases of a more general computation. In this framework, experience unfolding in time is encoded via a set of leaky integrators. These leaky integrators encode the Laplace transform of their input. The information contained in the transform can be recovered using an approximation to the inverse Laplace transform. In the temporal domain, the resulting representation reconstructs the temporal history. By integrating movements, the equations give rise to a representation of the path taken to arrive at the present location. By modulating the transform with information about allocentric velocity, the equations code for position of a landmark. Simulated cells show a close correspondence to neurons observed in various regions for all three cases. In the temporal domain, novel secondary analyses of hippocampal time cells verified several qualitative predictions of the model. An integrated representation of spatiotemporal context can be computed by taking conjunctions of these elemental inputs, leading to a correspondence with conjunctive neural representations observed in dorsal CA1.},
  number = {13},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J Neurosci},
  urldate = {2019-10-31},
  date = {2014-03-26},
  pages = {4692-4707},
  author = {Howard, Marc W. and MacDonald, Christopher J. and Tiganj, Zoran and Shankar, Karthik H. and Du, Qian and Hasselmo, Michael E. and Eichenbaum, Howard},
  file = {/home/alec/Zotero/storage/H8MR6Q26/Howard et al. - 2014 - A Unified Mathematical Framework for Coding Time, .pdf},
  eprinttype = {pmid},
  eprint = {24672015},
  pmcid = {PMC3965792}
}

@inproceedings{shankarGenericConstructionScaleInvariantly2015,
  langid = {english},
  location = {{Cham}},
  title = {Generic {{Construction}} of {{Scale}}-{{Invariantly Coarse Grained Memory}}},
  isbn = {978-3-319-14803-8},
  doi = {10.1007/978-3-319-14803-8_14},
  abstract = {Encoding temporal information from the recent past as spatially distributed activations is essential in order for the entire recent past to be simultaneously accessible. Any biological or synthetic agent that relies on the past to predict/plan the future, would be endowed with such a spatially distributed temporal memory. Simplistically, we would expect that resource limitations would demand the memory system to store only the most useful information for future prediction. For natural signals in real world which show scale free temporal fluctuations, the predictive information encoded in memory is maximal if the past information is scale invariantly coarse grained. Here we examine the general mechanism to construct a scale invariantly coarse grained memory system. Remarkably, the generic construction is equivalent to encoding the linear combinations of Laplace transform of the past information and their approximated inverses. This reveals a fundamental construction constraint on memory networks that attempt to maximize predictive information storage relevant to the natural world.},
  booktitle = {Artificial {{Life}} and {{Computational Intelligence}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  date = {2015},
  pages = {175-184},
  keywords = {inverse Laplace transformed memory,predictively relevant memory,resource conserving memory},
  author = {Shankar, Karthik H.},
  editor = {Chalup, Stephan K. and Blair, Alan D. and Randall, Marcus},
  file = {/home/alec/Zotero/storage/FJS3EG3J/Shankar - 2015 - Generic Construction of Scale-Invariantly Coarse G.pdf}
}

@article{howardEfficientNeuralComputation,
  langid = {english},
  title = {Efficient Neural Computation in the {{Laplace}} Domain},
  abstract = {Cognitive computation ought to be fast, efficient and flexible, reusing the same neural mechanisms to operate on many different forms of information. In order to develop neural models for cognitive computation we need to develop neurallyplausible implementations of fundamental operations. If the operations can be applied across sensory modalities, this requires a common form of neural coding. Weber-Fechner scaling is a general representational motif that is exploited by the brain not only in vision and audition, but also for efficient representations of time, space and numerosity. That is, for these variables, the brain appears to support functions f (x) by placing receptors at locations xi such that xi − xi−1 ∝ xi. The existence of a common form of neural representation suggests the possibility of a common form of cognitive computation across information domains. Efficient Weber-Fechner representations of time, space and number can be constructed using the Laplace transform, which can be inverted using a neurally-plausible matrix operation. Access to the Laplace domain allows for a range of efficient computations that can be performed on Weber-Fechner scaled representations. For instance, translation of a function f (x) by an amount δ to give f (x + δ) can be readily accomplished in the Laplace domain. We have worked out a neurally-plausible mapping hypothesis between translation and theta oscillations. Other operations, such as convolution and cross-correlation are extremely efficient in the Laplace domain, enabling the computation of addition and subtraction of neural representations. Implementation of neural circuits for these elemental computations would allow hybrid neural-symbolic architectures that exhibit properties such as compositionality and productivity.},
  pages = {8},
  author = {Howard, Marc W and Shankar, Karthik H and Tiganj, Zoran},
  file = {/home/alec/Zotero/storage/AIL9KHK3/Howard et al. - Efﬁcient neural computation in the Laplace domain.pdf}
}

@article{howardDistributedRepresentationInternal2015,
  title = {A Distributed Representation of Internal Time},
  volume = {122},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10.1037/a0037840},
  abstract = {This article pursues the hypothesis that a scale-invariant representation of history could support performance in a variety of learning and memory tasks. This representation maintains a conjunctive representation of what happened when that grows continuously less accurate for events further and further in the past. Simple behavioral models using a few operations, including scanning, matching and a “jump back in time” that recovers previous states of the history, describe a range of behavioral phenomena. These behavioral applications include canonical results from the judgment of recency task over short and long scales, the recency and contiguity effect across scales in episodic recall, and temporal mapping phenomena in conditioning. A growing body of neural data suggests that neural representations in several brain regions have qualitative properties predicted by the representation of temporal history. Taken together, these results suggest that a scale-invariant representation of temporal history may serve as a cornerstone of a physical model of cognition in learning and memory. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  number = {1},
  journaltitle = {Psychological Review},
  date = {2015},
  pages = {24-53},
  keywords = {Learning,Episodic Memory,History,Models,Time Perception},
  author = {Howard, Marc W. and Shankar, Karthik H. and Aue, William R. and Criss, Amy H.},
  file = {/home/alec/Zotero/storage/J4HUN5XW/2014-44142-001.html}
}

@article{tiganjSequentialFiringCodes2017,
  langid = {english},
  title = {Sequential {{Firing Codes}} for {{Time}} in {{Rodent Medial Prefrontal Cortex}}},
  volume = {27},
  issn = {1047-3211},
  url = {https://academic.oup.com/cercor/article/27/12/5663/2557339},
  doi = {10.1093/cercor/bhw336},
  abstract = {Abstract.  A subset of hippocampal neurons, known as “time cells” fire sequentially
          for circumscribed periods of time within a delay interval. We inve},
  number = {12},
  journaltitle = {Cerebral Cortex},
  shortjournal = {Cereb Cortex},
  urldate = {2019-10-31},
  date = {2017-12-01},
  pages = {5663-5671},
  author = {Tiganj, Zoran and Jung, Min Whan and Kim, Jieun and Howard, Marc W.},
  file = {/home/alec/Zotero/storage/VIHUN6LX/Tiganj et al. - 2017 - Sequential Firing Codes for Time in Rodent Medial .pdf;/home/alec/Zotero/storage/G97P2HT6/2557339.html}
}

@article{howardMemoryPerceptionCompressed2018,
  langid = {english},
  title = {Memory as {{Perception}} of the {{Past}}: {{Compressed Time inMind}} and {{Brain}}},
  volume = {22},
  issn = {1364-6613},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661317302449},
  doi = {10.1016/j.tics.2017.11.004},
  shorttitle = {Memory as {{Perception}} of the {{Past}}},
  abstract = {In the visual system retinal space is compressed such that acuity decreases further from the fovea. Different forms of memory may rely on a compressed representation of time, manifested as decreased accuracy for events that happened further in the past. Neurophysiologically, “time cells” show receptive fields in time. Analogous to the compression of visual space, time cells show less acuity for events further in the past. Behavioral evidence suggests memory can be accessed by scanning a compressed temporal representation, analogous to visual search. This suggests a common computational language for visual attention and memory retrieval. In this view, time functions like a scaffolding that organizes memories in much the same way that retinal space functions like a scaffolding for visual perception.},
  number = {2},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  urldate = {2019-10-31},
  date = {2018-02-01},
  pages = {124-136},
  keywords = {Memory,attention,time,Vision},
  author = {Howard, Marc W.},
  file = {/home/alec/Zotero/storage/R3LDXNEQ/Howard - 2018 - Memory as Perception of the Past Compressed Time .pdf;/home/alec/Zotero/storage/E42XY8RY/S1364661317302449.html}
}

@article{tiganjEstimatingScaleInvariantFuture2019,
  title = {Estimating {{Scale}}-{{Invariant Future}} in {{Continuous Time}}},
  volume = {31},
  issn = {0899-7667},
  url = {https://doi.org/10.1162/neco_a_01171},
  doi = {10.1162/neco_a_01171},
  abstract = {Natural learners must compute an estimate of future outcomes that follow from a stimulus in continuous time. Widely used reinforcement learning algorithms discretize continuous time and estimate either transition functions from one step to the next (model-based algorithms) or a scalar value of exponentially discounted future reward using the Bellman equation (model-free algorithms). An important drawback of model-based algorithms is that computational cost grows linearly with the amount of time to be simulated. An important drawback of model-free algorithms is the need to select a timescale required for exponential discounting. We present a computational mechanism, developed based on work in psychology and neuroscience, for computing a scale-invariant timeline of future outcomes. This mechanism efficiently computes an estimate of inputs as a function of future time on a logarithmically compressed scale and can be used to generate a scale-invariant power-law-discounted estimate of expected future reward. The representation of future time retains information about what will happen when. The entire timeline can be constructed in a single parallel operation that generates concrete behavioral and neural predictions. This computational mechanism could be incorporated into future reinforcement learning algorithms.},
  number = {4},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2019-10-31},
  date = {2019-02-14},
  pages = {681-709},
  author = {Tiganj, Zoran and Gershman, Samuel J. and Sederberg, Per B. and Howard, Marc W.},
  file = {/home/alec/Zotero/storage/P7Q2QVIC/neco_a_01171.html}
}

@article{liuNeuralMicrocircuitModel2019,
  langid = {english},
  title = {A Neural Microcircuit Model for a Scalable Scale-Invariant Representation of Time},
  volume = {29},
  issn = {1098-1063},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hipo.22994},
  doi = {10.1002/hipo.22994},
  abstract = {Scale-invariant timing has been observed in a wide range of behavioral experiments. The firing properties of recently described time cells provide a possible neural substrate for scale-invariant behavior. Earlier neural circuit models do not produce scale-invariant neural sequences. In this article, we present a biologically detailed network model based on an earlier mathematical algorithm. The simulations incorporate exponentially decaying persistent firing maintained by the calcium-activated nonspecific (CAN) cationic current and a network structure given by the inverse Laplace transform to generate time cells with scale-invariant firing rates. This model provides the first biologically detailed neural circuit for generating scale-invariant time cells. The circuit that implements the inverse Laplace transform merely consists of off-center/on-surround receptive fields. Critically, rescaling temporal sequences can be accomplished simply via cortical gain control (changing the slope of the f–I curve).},
  number = {3},
  journaltitle = {Hippocampus},
  urldate = {2019-10-31},
  date = {2019},
  pages = {260-274},
  keywords = {CAN-current,Laplace transform,rescaling,scale-invariance,time cells},
  author = {Liu, Yue and Tiganj, Zoran and Hasselmo, Michael E. and Howard, Marc W.},
  file = {/home/alec/Zotero/storage/J4KZXGUV/Liu et al. - 2019 - A neural microcircuit model for a scalable scale-i.pdf;/home/alec/Zotero/storage/I4KFG7SM/hipo.html}
}

@article{howardEvidenceAccumulationLaplace2018,
  langid = {english},
  title = {Evidence {{Accumulation}} in a {{Laplace Domain Decision Space}}},
  volume = {1},
  issn = {2522-087X},
  url = {https://doi.org/10.1007/s42113-018-0016-2},
  doi = {10.1007/s42113-018-0016-2},
  abstract = {Evidence accumulation models of simple decision-making have long assumed that the brain estimates a scalar decision variable corresponding to the log likelihood ratio of the two alternatives. Typical neural implementations of this algorithmic cognitive model assume that large numbers of neurons are each noisy exemplars of the scalar decision variable. Here, we propose a neural implementation of the diffusion model in which many neurons construct and maintain the Laplace transform of the distance to each of the decision bounds. As in classic findings from brain regions including LIP, the firing rate of neurons coding for the Laplace transform of net accumulated evidence grows to a bound during random dot motion tasks. However, rather than noisy exemplars of a single mean value, this approach makes the novel prediction that firing rates grow to the bound exponentially; across neurons, there should be a distribution of different rates. A second set of neurons records an approximate inversion of the Laplace transform; these neurons directly estimate net accumulated evidence. In analogy to time cells and place cells observed in the hippocampus and other brain regions, the neurons in this second set have receptive fields along a “decision axis.” This finding is consistent with recent findings from rodent recordings. This theoretical approach places simple evidence accumulation models in the same mathematical language as recent proposals for representing time and space in cognitive models for memory.},
  number = {3},
  journaltitle = {Computational Brain \& Behavior},
  shortjournal = {Comput Brain Behav},
  urldate = {2019-10-31},
  date = {2018-12-01},
  pages = {237-251},
  keywords = {Laplace transform,Diffusion model,Evidence accumulation,Neurophysiological models of cognition},
  author = {Howard, Marc W. and Luzardo, Andre and Tiganj, Zoran},
  file = {/home/alec/Zotero/storage/2A4CW7QA/Howard et al. - 2018 - Evidence Accumulation in a Laplace Domain Decision.pdf}
}

@article{simardBestPracticesConvolutional2003,
  langid = {american},
  title = {Best {{Practices}} for {{Convolutional Neural Networks Applied}} to {{Visual Document Analysis}}},
  url = {https://www.microsoft.com/en-us/research/publication/best-practices-for-convolutional-neural-networks-applied-to-visual-document-analysis/},
  abstract = {Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural …},
  urldate = {2019-11-13},
  date = {2003-08-01},
  author = {Simard, Patrice Y. and Steinkraus, Dave and Platt, John},
  file = {/home/alec/Zotero/storage/WVRCC4SR/Simard et al. - 2003 - Best Practices for Convolutional Neural Networks A.pdf;/home/alec/Zotero/storage/EJKCNVFC/best-practices-for-convolutional-neural-networks-applied-to-visual-document-analysis.html}
}

@article{ciresanDeepBigSimple2010,
  title = {Deep, {{Big}}, {{Simple Neural Nets}} for {{Handwritten Digit Recognition}}},
  volume = {22},
  issn = {0899-7667},
  url = {https://www.mitpressjournals.org/doi/10.1162/NECO_a_00052},
  doi = {10.1162/NECO_a_00052},
  abstract = {Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35\% error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.},
  number = {12},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  urldate = {2019-11-13},
  date = {2010-09-21},
  pages = {3207-3220},
  author = {Cireşan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, Jürgen},
  file = {/home/alec/Zotero/storage/NXJ6DP65/Cireşan et al. - 2010 - Deep, Big, Simple Neural Nets for Handwritten Digi.pdf;/home/alec/Zotero/storage/2G7SW8E9/NECO_a_00052.html}
}

@inproceedings{ciresanMulticolumnDeepNeural2012,
  title = {Multi-Column Deep Neural Networks for Image Classification},
  doi = {10.1109/CVPR.2012.6248110},
  abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
  eventtitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  date = {2012-06},
  pages = {3642-3649},
  keywords = {Neurons,neural nets,artificial neural network architectures,Benchmark testing,Computer architecture,computer vision,convolutional winner-take-all neurons,Error analysis,fast training,graphics cards,Graphics processing unit,graphics processing units,handwritten character recognition,handwritten digits recognition,human performance,image classification,image recognition,learning (artificial intelligence),machine learning,MNIST handwriting benchmark,multicolumn deep neural networks,retina,sparsely connected neural layers,traffic sign recognition benchmark,traffic signs,Training,visual cortex},
  author = {Cireşan, Dan and Meier, Ueli and Schmidhuber, Jürgen},
  file = {/home/alec/Zotero/storage/6QBK4ULG/Ciregan et al. - 2012 - Multi-column deep neural networks for image classi.pdf;/home/alec/Zotero/storage/M65P8PGZ/6248110.html},
  issn = {1063-6919, 1063-6919}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  volume = {86},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  number = {11},
  journaltitle = {Proceedings of the IEEE},
  date = {1998-11},
  pages = {2278-2324},
  keywords = {Neural networks,handwritten character recognition,2D shape variability,back-propagation,backpropagation,Character recognition,cheque reading,complex decision surface synthesis,convolution,convolutional neural network character recognizers,document recognition,document recognition systems,Feature extraction,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,GTN,handwritten digit recognition task,Hidden Markov models,high-dimensional patterns,language modeling,Machine learning,Multi-layer neural network,multilayer neural networks,multilayer perceptrons,multimodule systems,optical character recognition,Optical character recognition software,Optical computing,Pattern recognition,performance measure minimization,Principal component analysis,segmentation recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  file = {/home/alec/Zotero/storage/PSNPPBKT/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;/home/alec/Zotero/storage/552UZJW9/726791.html}
}

@article{keysersDeformationModelsImage2007,
  title = {Deformation {{Models}} for {{Image Recognition}}},
  volume = {29},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2007.1153},
  abstract = {We present the application of different nonlinear image deformation models to the task of image recognition. The deformation models are especially suited for local changes as they often occur in the presence of image object variability. We show that, among the discussed models, there is one approach that combines simplicity of implementation, low-computational complexity, and highly competitive performance across various real-world image recognition tasks. We show experimentally that the model performs very well for four different handwritten digit recognition tasks and for the classification of medical images, thus showing high generalization capacity. In particular, an error rate of 0.54 percent on the MNIST benchmark is achieved, as well as the lowest reported error rate, specifically 12.6 percent, in the 2005 international ImageCLEF evaluation of medical image specifically categorization.},
  number = {8},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  date = {2007-08},
  pages = {1422-1435},
  keywords = {Humans,Computer Simulation,Algorithms,Image Processing; Computer-Assisted,Error analysis,handwritten character recognition,image recognition,Character recognition,Artificial Intelligence,Biomedical imaging,character recognition,Classification algorithms,Context modeling,Deformable models,Handwriting recognition,handwritten digit recognition,image alignment,Image Interpretation; Computer-Assisted,Image matching,image object variability,Image recognition,low-computational complexity,medical image categorization,medical image categorization.,medical image processing,medical images classification,MNIST benchmark,Nonlinear Dynamics,nonlinear image deformation,Pattern Recognition; Automated,Pixel},
  author = {Keysers, Daniel and Deselaers, Thomas and Gollan, Christian and Ney, Hermann},
  file = {/home/alec/Zotero/storage/IYG37WCF/Keysers et al. - 2007 - Deformation Models for Image Recognition.pdf;/home/alec/Zotero/storage/ULTZQKMV/4250467.html}
}

@article{decosteTrainingInvariantSupport2002,
  langid = {english},
  title = {Training {{Invariant Support Vector Machines}}},
  volume = {46},
  issn = {1573-0565},
  url = {https://doi.org/10.1023/A:1012454411458},
  doi = {10.1023/A:1012454411458},
  abstract = {Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods.},
  number = {1},
  journaltitle = {Machine Learning},
  shortjournal = {Machine Learning},
  urldate = {2019-11-13},
  date = {2002-01-01},
  pages = {161-190},
  keywords = {image classification,invariance,pattern recognition,prior knowledge,support vector machines},
  author = {Decoste, Dennis and Schölkopf, Bernhard},
  file = {/home/alec/Zotero/storage/PTJW7WF3/Decoste and Schölkopf - 2002 - Training Invariant Support Vector Machines.pdf}
}

@article{yadavColdCaseLost2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.10498},
  primaryClass = {cs, stat},
  title = {Cold {{Case}}: {{The Lost MNIST Digits}}},
  url = {http://arxiv.org/abs/1905.10498},
  shorttitle = {Cold {{Case}}},
  abstract = {Although the popular MNIST dataset [LeCun et al., 1994] is derived from the NIST database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy. We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they enable us to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our results unambiguously confirm the trends observed by Recht et al. [2018, 2019]: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.},
  urldate = {2019-11-13},
  date = {2019-11-04},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Computer Vision and Pattern Recognition},
  author = {Yadav, Chhavi and Bottou, Léon},
  file = {/home/alec/Zotero/storage/UHR56CZG/Yadav and Bottou - 2019 - Cold Case The Lost MNIST Digits.pdf;/home/alec/Zotero/storage/V3BN2P3P/1905.html}
}

@article{kussulImprovedMethodHandwritten2004,
  langid = {english},
  title = {Improved Method of Handwritten Digit Recognition Tested on {{MNIST}} Database},
  volume = {22},
  issn = {0262-8856},
  url = {http://www.sciencedirect.com/science/article/pii/S0262885604000721},
  doi = {10.1016/j.imavis.2004.03.008},
  abstract = {We have developed a novel neural classifier LImited Receptive Area (LIRA) for the image recognition. The classifier LIRA contains three neuron layers: sensor, associative and output layers. The sensor layer is connected with the associative layer with no modifiable random connections and the associative layer is connected with the output layer with trainable connections. The training process converges sufficiently fast. This classifier does not use floating point and multiplication operations. The classifier was tested on two image databases. The first database is the MNIST database. It contains 60,000 handwritten digit images for the classifier training and 10,000 handwritten digit images for the classifier testing. The second database contains 441 images of the assembly microdevice. The problem under investigation is to recognize the position of the pin relatively to the hole. A random procedure was used for partition of the database to training and testing subsets. There are many results for the MNIST database in the literature. In the best cases, the error rates are 0.7, 0.63 and 0.42\%. The classifier LIRA gives error rate of 0.61\% as a mean value of three trials. In task of the pin–hole position estimation the classifier LIRA also shows sufficiently good results.},
  number = {12},
  journaltitle = {Image and Vision Computing},
  shortjournal = {Image and Vision Computing},
  series = {Proceedings from the 15th {{International Conference}} on {{Vision Interface}}},
  urldate = {2019-11-13},
  date = {2004-10-01},
  pages = {971-981},
  keywords = {Handwritten digit recognition,LImited Receptive Area neural classifier,Microdevice assembly,MNIST database},
  author = {Kussul, Ernst and Baidyk, Tatiana},
  file = {/home/alec/Zotero/storage/GBX6AYWE/Kussul and Baidyk - 2004 - Improved method of handwritten digit recognition t.pdf;/home/alec/Zotero/storage/E7FWT3DP/S0262885604000721.html}
}

@inproceedings{bottouComparisonClassifierMethods1994,
  title = {Comparison of Classifier Methods: A Case Study in Handwritten Digit Recognition},
  volume = {2},
  doi = {10.1109/ICPR.1994.576879},
  shorttitle = {Comparison of Classifier Methods},
  abstract = {This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.},
  eventtitle = {Proceedings of the 12th {{IAPR International Conference}} on {{Pattern Recognition}}, {{Vol}}. 3 - {{Conference C}}: {{Signal Processing}} ({{Cat}}. {{No}}.{{94CH3440}}-5)},
  booktitle = {Proceedings of the 12th {{IAPR International Conference}} on {{Pattern Recognition}}, {{Vol}}. 3 - {{Conference C}}: {{Signal Processing}} ({{Cat}}. {{No}}.{{94CH3440}}-5)},
  date = {1994-10},
  pages = {77-82 vol.2},
  keywords = {Testing,learning,Machine learning,multilayer neural networks,Pattern recognition,character recognition,Handwriting recognition,handwritten digit recognition,Computer aided software engineering,database,Databases,image classifier,Laboratories,LeNet 1,LeNet 4,NIST,recognition time,Time measurement,Training data,training time},
  author = {Bottou, L. and Cortes, C. and Denker, J.S. and Drucker, H. and Guyon, I. and Jackel, L.D. and LeCun, Y. and Muller, U.A. and Sackinger, E. and Simard, P. and Vapnik, V.},
  file = {/home/alec/Zotero/storage/7UB39SLV/Bottou et al. - 1994 - Comparison of classifier methods a case study in .pdf;/home/alec/Zotero/storage/RP7WFA7J/576879.html}
}

@online{krishnakantgujarHandwrittenDigitDatabase,
  title = {Handwritten Digit Database},
  url = {http://cis.jhu.edu/~sachin/digit/digit.html},
  urldate = {2019-11-13},
  author = {Krishnakant Gujar, Sachin},
  file = {/home/alec/Zotero/storage/RUQ3XH9F/digit.html}
}

@article{kingmaAdamMethodStochastic2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  url = {http://arxiv.org/abs/1412.6980},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate = {2019-11-13},
  date = {2017-01-29},
  keywords = {Computer Science - Machine Learning},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  file = {/home/alec/Zotero/storage/Q9SQKSV2/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/alec/Zotero/storage/9RVKYCH5/1412.html}
}

@article{innesFluxElegantMachine2018,
  title = {Flux: {{Elegant}} Machine Learning with {{Julia}}},
  volume = {3},
  doi = {10.21105/joss.00602},
  shorttitle = {Flux},
  journaltitle = {Journal of Open Source Software},
  shortjournal = {Journal of Open Source Software},
  date = {2018-05-03},
  pages = {602},
  author = {Innes, Michael},
  file = {/home/alec/Zotero/storage/5E4E68A4/Innes - 2018 - Flux Elegant machine learning with Julia.pdf}
}


